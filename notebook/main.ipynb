{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76ee48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c7cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, scaler=None, fit_scaler=True):\n",
    "    \"\"\"\n",
    "    Preprocess the compressor data\n",
    "    \"\"\"\n",
    "    # Convert timestamp to datetime if it's not already\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.sort_values('timestamp')\n",
    "    \n",
    "    # Select only numeric columns (excluding timestamp and compressor_id)\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'compressor_id' in numeric_columns:\n",
    "        numeric_columns.remove('compressor_id')\n",
    "    \n",
    "    # Fill any missing values\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(method='ffill')\n",
    "    \n",
    "    # Initialize scaler if not provided\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    # Normalize the data\n",
    "    if fit_scaler:\n",
    "        scaled_data = scaler.fit_transform(df[numeric_columns])\n",
    "    else:\n",
    "        scaled_data = scaler.transform(df[numeric_columns])\n",
    "    \n",
    "    return scaled_data, numeric_columns, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760aabf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM training\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        sequences.append(data[i:(i + sequence_length)])\n",
    "    return np.array(sequences)\n",
    "\n",
    "\n",
    "def build_lstm_autoencoder(sequence_length, n_features, encoding_dim=32, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build the LSTM Autoencoder model\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(sequence_length, n_features))\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = LSTM(encoding_dim, activation='relu', return_sequences=False)(input_layer)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = RepeatVector(sequence_length)(encoded)\n",
    "    decoded = LSTM(encoding_dim, activation='relu', return_sequences=True)(decoded)\n",
    "    decoded = TimeDistributed(Dense(n_features, activation='linear'))(decoded)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = Model(input_layer, decoded)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                  loss='mse', \n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_on_normal_data(normal_df, sequence_length=10, encoding_dim=32, \n",
    "                        learning_rate=0.001, epochs=100, batch_size=32, \n",
    "                        validation_split=0.2, verbose=1):\n",
    "    \"\"\"\n",
    "    Train the LSTM autoencoder on NORMAL operating data only\n",
    "    \n",
    "    This teaches the model what \"normal\" looks like.\n",
    "    Any deviation from this learned pattern will have high reconstruction error.\n",
    "    \n",
    "    Returns:\n",
    "    - model: Trained autoencoder model\n",
    "    - scaler: Fitted data scaler\n",
    "    - threshold: Anomaly detection threshold\n",
    "    - feature_columns: List of feature column names\n",
    "    - history: Training history\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"TRAINING ON NORMAL DATA\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nPreprocessing normal operating data...\")\n",
    "    \n",
    "    # Preprocess normal data\n",
    "    scaled_data, feature_columns, scaler = preprocess_data(normal_df, fit_scaler=True)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = create_sequences(scaled_data, sequence_length)\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total samples: {len(normal_df)}\")\n",
    "    print(f\"  Features: {len(feature_columns)}\")\n",
    "    print(f\"  Sequences created: {len(sequences)}\")\n",
    "    print(f\"  Sequence shape: {sequences.shape}\")\n",
    "    print(f\"\\nFeatures being monitored:\")\n",
    "    for i, col in enumerate(feature_columns, 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_lstm_autoencoder(sequence_length, scaled_data.shape[1], \n",
    "                                  encoding_dim, learning_rate)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"Model Architecture:\")\n",
    "    print(\"=\"*70)\n",
    "    model.summary()\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train model on normal data\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training autoencoder to learn normal behavior...\")\n",
    "    print(\"=\"*70)\n",
    "    history = model.fit(\n",
    "        sequences, sequences, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        validation_split=validation_split,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=verbose,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Calculate threshold from training data\n",
    "    # We use validation split to set threshold to avoid overfitting\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Calculating anomaly threshold...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    train_predictions = model.predict(sequences, verbose=0)\n",
    "    train_mse = np.mean(np.power(sequences - train_predictions, 2), axis=(1, 2))\n",
    "    \n",
    "    # Use higher percentile (95-99) to minimize false positives\n",
    "    # Lower percentile = more sensitive (more anomalies detected)\n",
    "    # Higher percentile = less sensitive (fewer false alarms)\n",
    "    threshold_percentile = 99  # Changed from 95 to 99 for stricter threshold\n",
    "    threshold = np.percentile(train_mse, threshold_percentile)\n",
    "    \n",
    "    print(f\"\\nReconstruction Error on Normal Data:\")\n",
    "    print(f\"  Mean: {train_mse.mean():.6f}\")\n",
    "    print(f\"  Std:  {train_mse.std():.6f}\")\n",
    "    print(f\"  Min:  {train_mse.min():.6f}\")\n",
    "    print(f\"  Max:  {train_mse.max():.6f}\")\n",
    "    print(f\"  95th percentile: {np.percentile(train_mse, 95):.6f}\")\n",
    "    print(f\"  99th percentile: {np.percentile(train_mse, 99):.6f}\")\n",
    "    print(f\"\\nâœ“ Threshold set at {threshold_percentile}th percentile: {threshold:.6f}\")\n",
    "    print(f\"  (Any test data with reconstruction error > {threshold:.6f} is anomalous)\")\n",
    "    \n",
    "    return model, scaler, threshold, feature_columns, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21fbfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_data(model, scaler, threshold, feature_columns, test_df, \n",
    "                sequence_length=10, consecutive_threshold=3, data_label=\"Test\"):\n",
    "    \"\"\"\n",
    "    Test the trained model on new data (could be normal or anomalous)\n",
    "    \n",
    "    The model will compare test data to what it learned as \"normal\".\n",
    "    High reconstruction error = data doesn't match normal patterns = ANOMALY\n",
    "    \n",
    "    Parameters:\n",
    "    - consecutive_threshold: Number of consecutive anomalous points to trigger alert\n",
    "    - data_label: Label for this dataset (e.g., \"Normal Test\", \"Fault Condition\")\n",
    "    \n",
    "    Returns:\n",
    "    - results: DataFrame with anomaly detection results including feature-level errors\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TESTING ON {data_label.upper()} DATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Preprocess test data using SAME scaler from training\n",
    "    scaled_data, _, _ = preprocess_data(test_df, scaler=scaler, fit_scaler=False)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = create_sequences(scaled_data, sequence_length)\n",
    "    \n",
    "    print(f\"Test sequences created: {len(sequences)}\")\n",
    "    \n",
    "    # Get model predictions (reconstruction)\n",
    "    print(\"Reconstructing sequences...\")\n",
    "    predictions = model.predict(sequences, verbose=0)\n",
    "    \n",
    "    # Calculate overall reconstruction error (how different is test data from normal?)\n",
    "    mse = np.mean(np.power(sequences - predictions, 2), axis=(1, 2))\n",
    "    \n",
    "    # Calculate FEATURE-LEVEL reconstruction errors\n",
    "    # Shape: (n_sequences, n_features)\n",
    "    feature_mse = np.mean(np.power(sequences - predictions, 2), axis=1)\n",
    "    \n",
    "    # Determine individual anomalies (points exceeding threshold)\n",
    "    is_anomaly_point = mse > threshold\n",
    "    \n",
    "    # Apply consecutive threshold logic for robust detection\n",
    "    triggered_anomaly = np.zeros(len(mse), dtype=bool)\n",
    "    consecutive_count = 0\n",
    "    \n",
    "    for i in range(len(is_anomaly_point)):\n",
    "        if is_anomaly_point[i]:\n",
    "            consecutive_count += 1\n",
    "            # If we've hit consecutive threshold, mark all points in window as triggered\n",
    "            if consecutive_count >= consecutive_threshold:\n",
    "                start_idx = max(0, i - consecutive_count + 1)\n",
    "                triggered_anomaly[start_idx:i+1] = True\n",
    "        else:\n",
    "            consecutive_count = 0\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = pd.DataFrame({\n",
    "        'sequence_index': range(len(mse)),\n",
    "        'reconstruction_error': mse,\n",
    "        'threshold': threshold,\n",
    "        'is_anomaly_point': is_anomaly_point,\n",
    "        'triggered_anomaly': triggered_anomaly,\n",
    "        'anomaly_score': mse / threshold,  # >1.0 means anomalous\n",
    "        'consecutive_count': 0\n",
    "    })\n",
    "    \n",
    "    # Add feature-level errors as separate columns\n",
    "    for i, col in enumerate(feature_columns):\n",
    "        results[f'error_{col}'] = feature_mse[:, i]\n",
    "    \n",
    "    # Calculate consecutive count for each point\n",
    "    consecutive = 0\n",
    "    for i in range(len(results)):\n",
    "        if is_anomaly_point[i]:\n",
    "            consecutive += 1\n",
    "        else:\n",
    "            consecutive = 0\n",
    "        results.loc[i, 'consecutive_count'] = consecutive\n",
    "    \n",
    "    # Map back to original timestamps if available\n",
    "    if 'timestamp' in test_df.columns:\n",
    "        start_idx = sequence_length - 1\n",
    "        results['timestamp'] = test_df['timestamp'].iloc[start_idx:start_idx + len(results)].values\n",
    "    \n",
    "    # Print summary\n",
    "    print_test_summary(results, data_label, consecutive_threshold)\n",
    "    \n",
    "    # Analyze which features are most anomalous\n",
    "    analyze_anomalous_features(results, feature_columns, triggered_anomaly)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_anomalous_features(results, feature_columns, triggered_anomaly):\n",
    "    \"\"\"\n",
    "    Analyze which features contribute most to anomalies\n",
    "    \"\"\"\n",
    "    if triggered_anomaly.sum() == 0:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ANOMALOUS FEATURE ANALYSIS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get feature error columns\n",
    "    error_cols = [f'error_{col}' for col in feature_columns]\n",
    "    \n",
    "    # Calculate mean error for each feature during anomalies\n",
    "    anomaly_feature_errors = results[triggered_anomaly][error_cols].mean()\n",
    "    normal_feature_errors = results[~triggered_anomaly][error_cols].mean()\n",
    "    \n",
    "    # Calculate relative increase\n",
    "    feature_contribution = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'normal_error': normal_feature_errors.values,\n",
    "        'anomaly_error': anomaly_feature_errors.values,\n",
    "        'increase_ratio': (anomaly_feature_errors / (normal_feature_errors + 1e-10)).values,\n",
    "        'contribution_pct': 100 * anomaly_feature_errors / anomaly_feature_errors.sum()\n",
    "    })\n",
    "    \n",
    "    # Sort by contribution percentage\n",
    "    feature_contribution = feature_contribution.sort_values('contribution_pct', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop Contributing Features to Anomalies:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Rank':<6}{'Feature':<35}{'Contribution':<15}{'Increase':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, (idx, row) in enumerate(feature_contribution.head(10).iterrows(), 1):\n",
    "        print(f\"{i:<6}{row['feature']:<35}{row['contribution_pct']:>6.2f}%{' '*8}{row['increase_ratio']:>6.2f}x\")\n",
    "    \n",
    "    return feature_contribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_summary(results, data_label, consecutive_threshold):\n",
    "    \"\"\"\n",
    "    Print summary of test results\n",
    "    \"\"\"\n",
    "    total_points = len(results)\n",
    "    anomaly_points = results['is_anomaly_point'].sum()\n",
    "    triggered_points = results['triggered_anomaly'].sum()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RESULTS SUMMARY - {data_label}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total sequences analyzed: {total_points}\")\n",
    "    print(f\"Points exceeding threshold: {anomaly_points} ({100*anomaly_points/total_points:.2f}%)\")\n",
    "    print(f\"Triggered alerts (â‰¥{consecutive_threshold} consecutive): {triggered_points} ({100*triggered_points/total_points:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nReconstruction Error Statistics:\")\n",
    "    print(f\"  Mean:   {results['reconstruction_error'].mean():.6f}\")\n",
    "    print(f\"  Median: {results['reconstruction_error'].median():.6f}\")\n",
    "    print(f\"  Std:    {results['reconstruction_error'].std():.6f}\")\n",
    "    print(f\"  Min:    {results['reconstruction_error'].min():.6f}\")\n",
    "    print(f\"  Max:    {results['reconstruction_error'].max():.6f}\")\n",
    "    \n",
    "    print(f\"\\nAnomaly Score Statistics (score > 1.0 = anomaly):\")\n",
    "    print(f\"  Mean:   {results['anomaly_score'].mean():.3f}\")\n",
    "    print(f\"  Median: {results['anomaly_score'].median():.3f}\")\n",
    "    print(f\"  Max:    {results['anomaly_score'].max():.3f}\")\n",
    "    \n",
    "    if triggered_points > 0:\n",
    "        print(f\"\\nâš ï¸  ANOMALIES DETECTED!\")\n",
    "        print(f\"   {triggered_points} sequences triggered anomaly alerts\")\n",
    "        \n",
    "        # Show when anomalies occurred\n",
    "        triggered = results[results['triggered_anomaly']]\n",
    "        if 'timestamp' in triggered.columns:\n",
    "            print(f\"\\n   First anomaly: {triggered['timestamp'].iloc[0]}\")\n",
    "            print(f\"   Last anomaly:  {triggered['timestamp'].iloc[-1]}\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“ No anomalies detected - system operating normally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c93c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss (MSE)')\n",
    "    ax1.set_title('Model Loss - Learning Normal Behavior')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "    ax2.plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Mean Absolute Error')\n",
    "    ax2.set_title('Model MAE - Learning Normal Behavior')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52b303af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_contributions(results, feature_columns, title=\"Feature-Level Anomaly Analysis\"):\n",
    "    \"\"\"\n",
    "    Plot which features contribute most to anomalies\n",
    "    \"\"\"\n",
    "    # Get only triggered anomalies\n",
    "    if results['triggered_anomaly'].sum() == 0:\n",
    "        print(\"No anomalies to analyze\")\n",
    "        return None\n",
    "    \n",
    "    anomaly_data = results[results['triggered_anomaly']]\n",
    "    normal_data = results[~results['triggered_anomaly']]\n",
    "    \n",
    "    # Calculate mean errors per feature\n",
    "    error_cols = [f'error_{col}' for col in feature_columns]\n",
    "    \n",
    "    anomaly_errors = anomaly_data[error_cols].mean().values\n",
    "    normal_errors = normal_data[error_cols].mean().values if len(normal_data) > 0 else np.zeros(len(error_cols))\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Bar chart of feature contributions\n",
    "    x_pos = np.arange(len(feature_columns))\n",
    "    \n",
    "    ax1.bar(x_pos, anomaly_errors, alpha=0.7, color='red', label='Anomaly Periods')\n",
    "    ax1.bar(x_pos, normal_errors, alpha=0.7, color='green', label='Normal Periods')\n",
    "    \n",
    "    ax1.set_xlabel('Features', fontsize=12)\n",
    "    ax1.set_ylabel('Mean Reconstruction Error', fontsize=12)\n",
    "    ax1.set_title('Reconstruction Error by Feature', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(feature_columns, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Sorted contribution percentage\n",
    "    contribution_pct = 100 * anomaly_errors / anomaly_errors.sum()\n",
    "    sorted_idx = np.argsort(contribution_pct)[::-1]\n",
    "    \n",
    "    colors = ['red' if c > 100/len(feature_columns) else 'orange' for c in contribution_pct[sorted_idx]]\n",
    "    \n",
    "    ax2.barh(range(len(feature_columns)), contribution_pct[sorted_idx], color=colors, alpha=0.7)\n",
    "    ax2.set_yticks(range(len(feature_columns)))\n",
    "    ax2.set_yticklabels([feature_columns[i] for i in sorted_idx])\n",
    "    ax2.set_xlabel('Contribution to Total Anomaly (%)', fontsize=12)\n",
    "    ax2.set_title('Feature Contribution Ranking', fontsize=14, fontweight='bold')\n",
    "    ax2.axvline(x=100/len(feature_columns), color='black', linestyle='--', \n",
    "                linewidth=1, label=f'Average ({100/len(feature_columns):.1f}%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2f69772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_results(results, threshold, title=\"Anomaly Detection Results\", \n",
    "                     consecutive_threshold=3):\n",
    "    \"\"\"\n",
    "    Plot the test results showing anomaly detection\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Reconstruction Error\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(results.index, results['reconstruction_error'], \n",
    "             label='Reconstruction Error', alpha=0.8, color='blue', linewidth=1.5)\n",
    "    ax1.axhline(y=threshold, color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Threshold = {threshold:.4f}')\n",
    "    \n",
    "    # Shade individual anomaly points\n",
    "    ax1.fill_between(results.index, 0, results['reconstruction_error'], \n",
    "                     where=results['is_anomaly_point'], alpha=0.3, \n",
    "                     color='orange', label='Exceeds Threshold')\n",
    "    \n",
    "    # Highlight triggered anomalies\n",
    "    ax1.fill_between(results.index, 0, results['reconstruction_error'], \n",
    "                     where=results['triggered_anomaly'], alpha=0.5, \n",
    "                     color='red', label=f'TRIGGERED ALERT')\n",
    "    \n",
    "    ax1.set_ylabel('Reconstruction Error', fontsize=12)\n",
    "    ax1.set_title('Reconstruction Error: Deviation from Normal Behavior', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Anomaly Score (Normalized)\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(results.index, results['anomaly_score'], \n",
    "             label='Anomaly Score', color='darkorange', linewidth=1.5)\n",
    "    ax2.axhline(y=1.0, color='red', linestyle='--', linewidth=2,\n",
    "                label='Anomaly Threshold (1.0)')\n",
    "    \n",
    "    # Shade triggered anomalies\n",
    "    ax2.fill_between(results.index, 0, results['anomaly_score'], \n",
    "                     where=results['triggered_anomaly'], alpha=0.5, \n",
    "                     color='red', label=f'TRIGGERED ALERT')\n",
    "    \n",
    "    ax2.set_ylabel('Anomaly Score\\n(Reconstruction Error / Threshold)', fontsize=12)\n",
    "    ax2.set_title('Normalized Anomaly Score (>1.0 = Anomalous)', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Consecutive Anomaly Counter\n",
    "    ax3 = axes[2]\n",
    "    colors = ['green' if c < consecutive_threshold else 'red' \n",
    "              for c in results['consecutive_count']]\n",
    "    ax3.bar(results.index, results['consecutive_count'], \n",
    "            color=colors, alpha=0.6, width=1.0)\n",
    "    ax3.axhline(y=consecutive_threshold, color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Trigger Level = {consecutive_threshold}')\n",
    "    \n",
    "    ax3.set_ylabel('Consecutive\\nAnomalous Points', fontsize=12)\n",
    "    ax3.set_xlabel('Time Step (Sequence Index)', fontsize=12)\n",
    "    ax3.set_title(f'Consecutive Anomaly Counter (Alert when â‰¥ {consecutive_threshold})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax3.legend(loc='upper right')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d224f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main workflow:\n",
    "    1. Train on ALL normal data to learn normal behavior\n",
    "    2. Test on separate data (normal or anomalous) to detect deviations\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" LSTM AUTOENCODER - ANOMALY DETECTION FOR COMPRESSOR \")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nWorkflow:\")\n",
    "    print(\"  1. Train on NORMAL operating data (learn baseline)\")\n",
    "    print(\"  2. Test on new data (detect deviations from baseline)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 1: TRAIN ON NORMAL DATA\n",
    "    # ==========================================\n",
    "    print(\"\\nğŸ“Š Loading normal operating data...\")\n",
    "    normal_df = pd.read_csv(\"C:/Users/adminuser/Documents/compressor_normal_dataset3.csv\")  \n",
    "    print(f\"   Loaded {len(normal_df)} normal samples\")\n",
    "    \n",
    "    # Training parameters\n",
    "    sequence_length = 10\n",
    "    consecutive_threshold = 4  # Need 4 consecutive anomalies to trigger alert\n",
    "    \n",
    "    # Train model to learn normal behavior\n",
    "    model, scaler, threshold, feature_columns, history = train_on_normal_data(\n",
    "        normal_df,\n",
    "        sequence_length=sequence_length,\n",
    "        encoding_dim=16,\n",
    "        learning_rate=0.001,\n",
    "        epochs=50,\n",
    "        batch_size=16,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training progress\n",
    "    print(\"\\nğŸ“ˆ Plotting training history...\")\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 2: TEST ON NEW DATA\n",
    "    # ==========================================\n",
    "    \n",
    "    # Option A: Test on normal data (should have low reconstruction error)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TESTING PHASE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nğŸ” Testing on normal data (validation check)...\")\n",
    "    \n",
    "    # Use a portion of normal data for validation\n",
    "    test_normal_df = pd.read_csv(\"D:\\compressor_E_dataset.csv\")\n",
    "\n",
    "    #test_normal_df = normal_df.iloc[int(0.8*len(normal_df)):].copy()\n",
    "    \n",
    "    results_normal = test_on_data(\n",
    "        model, scaler, threshold, feature_columns,\n",
    "        test_normal_df,\n",
    "        sequence_length=sequence_length,\n",
    "        consecutive_threshold=consecutive_threshold,\n",
    "        data_label=\"Normal Data (Validation)\"\n",
    "    )\n",
    "    \n",
    "    plot_test_results(results_normal, threshold,\n",
    "                     title=\"Testing on Normal Data - Should Have Low Errors\",\n",
    "                     consecutive_threshold=consecutive_threshold)\n",
    "    \n",
    "    # Option B: Test on anomalous data (you need to provide this)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TO TEST ON ANOMALOUS DATA:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nLoad your fault/anomaly dataset and run:\")\n",
    "    print(\"```python\")\n",
    "    print(\"anomaly_df = pd.read_csv('your_anomaly_data.csv')\")\n",
    "    print(\"results_anomaly = test_on_data(\")\n",
    "    print(\"    model, scaler, threshold, feature_columns,\")\n",
    "    print(\"    anomaly_df,\")\n",
    "    print(\"    sequence_length=sequence_length,\")\n",
    "    print(\"    consecutive_threshold=consecutive_threshold,\")\n",
    "    print(\"    data_label='Fault Condition'\")\n",
    "    print(\")\")\n",
    "    print(\"plot_test_results(results_anomaly, threshold,\")\n",
    "    print(\"                 title='Testing on Fault Data',\")\n",
    "    print(\"                 consecutive_threshold=consecutive_threshold)\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    return model, scaler, threshold, feature_columns, results_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7078037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " LSTM AUTOENCODER - ANOMALY DETECTION FOR COMPRESSOR \n",
      "======================================================================\n",
      "\n",
      "Workflow:\n",
      "  1. Train on NORMAL operating data (learn baseline)\n",
      "  2. Test on new data (detect deviations from baseline)\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Loading normal operating data...\n",
      "   Loaded 30000 normal samples\n",
      "======================================================================\n",
      "TRAINING ON NORMAL DATA\n",
      "======================================================================\n",
      "\n",
      "Preprocessing normal operating data...\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total samples: 30000\n",
      "  Features: 11\n",
      "  Sequences created: 29991\n",
      "  Sequence shape: (29991, 10, 11)\n",
      "\n",
      "Features being monitored:\n",
      "  1. filter_dp\n",
      "  2. seal_gas_flow\n",
      "  3. seal_gas_diff_pressure\n",
      "  4. seal_gas_temp\n",
      "  5. primary_vent_flow\n",
      "  6. primary_vent_pressure\n",
      "  7. secondary_seal_gas_flow\n",
      "  8. separation_seal_gas_flow\n",
      "  9. separation_seal_gas_pressure\n",
      "  10. seal_gas_to_vent_diff_pressure\n",
      "  11. encoding\n",
      "\n",
      "======================================================================\n",
      "Model Architecture:\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ repeat_vector_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ time_distributed_1              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)         â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">187</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               â”‚                        â”‚               â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m11\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             â”‚         \u001b[38;5;34m1,792\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ repeat_vector_1 (\u001b[38;5;33mRepeatVector\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)         â”‚         \u001b[38;5;34m2,112\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ time_distributed_1              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m11\u001b[0m)         â”‚           \u001b[38;5;34m187\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mTimeDistributed\u001b[0m)               â”‚                        â”‚               â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,091</span> (15.98 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,091\u001b[0m (15.98 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,091</span> (15.98 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,091\u001b[0m (15.98 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training autoencoder to learn normal behavior...\n",
      "======================================================================\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     model, scaler, threshold, feature_columns, results = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Actually save the trained model for future use\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     24\u001b[39m consecutive_threshold = \u001b[32m4\u001b[39m  \u001b[38;5;66;03m# Need 4 consecutive anomalies to trigger alert\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Train model to learn normal behavior\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m model, scaler, threshold, feature_columns, history = \u001b[43mtrain_on_normal_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormal_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     36\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Plot training progress\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ“ˆ Plotting training history...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mtrain_on_normal_data\u001b[39m\u001b[34m(normal_df, sequence_length, encoding_dim, learning_rate, epochs, batch_size, validation_split, verbose)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining autoencoder to learn normal behavior...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     95\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Calculate threshold from training data\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# We use validation split to set threshold to avoid overfitting\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:889\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    886\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    887\u001b[39m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[32m    888\u001b[39m   initializers = []\n\u001b[32m--> \u001b[39m\u001b[32m889\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    891\u001b[39m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[32m    892\u001b[39m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[32m    893\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[39m, in \u001b[36mFunction._initialize\u001b[39m\u001b[34m(self, args, kwds, add_initializers_to)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28mself\u001b[39m._variable_creation_config = \u001b[38;5;28mself\u001b[39m._generate_scoped_tracing_options(\n\u001b[32m    692\u001b[39m     variable_capturing_scope,\n\u001b[32m    693\u001b[39m     tracing_compilation.ScopeType.VARIABLE_CREATION,\n\u001b[32m    694\u001b[39m )\n\u001b[32m    695\u001b[39m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvalid_creator_scope\u001b[39m(*unused_args, **unused_kwds):\n\u001b[32m    701\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[39m, in \u001b[36mtrace_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    175\u001b[39m     args = tracing_options.input_signature\n\u001b[32m    176\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m   concrete_function = \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options.bind_graph_to_function:\n\u001b[32m    183\u001b[39m   concrete_function._garbage_collector.release()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[39m, in \u001b[36m_maybe_define_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    282\u001b[39m   target_func_type = lookup_func_type\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m concrete_function = \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tracing_options.function_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    288\u001b[39m   tracing_options.function_cache.add(\n\u001b[32m    289\u001b[39m       concrete_function, current_func_context\n\u001b[32m    290\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[39m, in \u001b[36m_create_concrete_function\u001b[39m\u001b[34m(function_type, type_context, func_graph, tracing_options)\u001b[39m\n\u001b[32m    303\u001b[39m   placeholder_bound_args = function_type.placeholder_arguments(\n\u001b[32m    304\u001b[39m       placeholder_context\n\u001b[32m    305\u001b[39m   )\n\u001b[32m    307\u001b[39m disable_acd = tracing_options.attributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options.attributes.get(\n\u001b[32m    308\u001b[39m     attributes_lib.DISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    309\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m traced_func_graph = \u001b[43mfunc_graph_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction_type_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m transform.apply_func_graph_transforms(traced_func_graph)\n\u001b[32m    324\u001b[39m graph_capture_container = traced_func_graph.function_captures\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1060\u001b[39m, in \u001b[36mfunc_graph_from_py_func\u001b[39m\u001b[34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[39m\n\u001b[32m   1057\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m   1059\u001b[39m _, original_func = tf_decorator.unwrap(python_func)\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m func_outputs = \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[32m   1064\u001b[39m func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[39m, in \u001b[36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m default_graph._variable_creator_scope(scope, priority=\u001b[32m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    596\u001b[39m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[32m    597\u001b[39m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[32m    598\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     out = \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[39m, in \u001b[36mpy_func_from_autograph.<locals>.autograph_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m     51\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mag_error_metadata\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339\u001b[39m, in \u001b[36mconverted_call\u001b[39m\u001b[34m(f, args, kwargs, caller_fn_scope, options)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[32m    338\u001b[39m   logging.log(\u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: AutoGraph artifact\u001b[39m\u001b[33m'\u001b[39m, f)\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools.partial):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[39m, in \u001b[36m_call_unconverted\u001b[39m\u001b[34m(f, args, kwargs, options, update_cache)\u001b[39m\n\u001b[32m    456\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f.\u001b[34m__self__\u001b[39m.call(args, kwargs)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m f(*args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:133\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmulti_step_on_iterator\u001b[39m(iterator):\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps_per_execution == \u001b[32m1\u001b[39m:\n\u001b[32m    132\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tf.experimental.Optional.from_value(\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m             \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m         )\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n\u001b[32m    137\u001b[39m     empty_outputs = tf.experimental.Optional.empty(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:889\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    886\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    887\u001b[39m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[32m    888\u001b[39m   initializers = []\n\u001b[32m--> \u001b[39m\u001b[32m889\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    891\u001b[39m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[32m    892\u001b[39m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[32m    893\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[39m, in \u001b[36mFunction._initialize\u001b[39m\u001b[34m(self, args, kwds, add_initializers_to)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28mself\u001b[39m._variable_creation_config = \u001b[38;5;28mself\u001b[39m._generate_scoped_tracing_options(\n\u001b[32m    692\u001b[39m     variable_capturing_scope,\n\u001b[32m    693\u001b[39m     tracing_compilation.ScopeType.VARIABLE_CREATION,\n\u001b[32m    694\u001b[39m )\n\u001b[32m    695\u001b[39m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvalid_creator_scope\u001b[39m(*unused_args, **unused_kwds):\n\u001b[32m    701\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[39m, in \u001b[36mtrace_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    175\u001b[39m     args = tracing_options.input_signature\n\u001b[32m    176\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m   concrete_function = \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options.bind_graph_to_function:\n\u001b[32m    183\u001b[39m   concrete_function._garbage_collector.release()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[39m, in \u001b[36m_maybe_define_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    282\u001b[39m   target_func_type = lookup_func_type\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m concrete_function = \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tracing_options.function_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    288\u001b[39m   tracing_options.function_cache.add(\n\u001b[32m    289\u001b[39m       concrete_function, current_func_context\n\u001b[32m    290\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[39m, in \u001b[36m_create_concrete_function\u001b[39m\u001b[34m(function_type, type_context, func_graph, tracing_options)\u001b[39m\n\u001b[32m    303\u001b[39m   placeholder_bound_args = function_type.placeholder_arguments(\n\u001b[32m    304\u001b[39m       placeholder_context\n\u001b[32m    305\u001b[39m   )\n\u001b[32m    307\u001b[39m disable_acd = tracing_options.attributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options.attributes.get(\n\u001b[32m    308\u001b[39m     attributes_lib.DISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    309\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m traced_func_graph = \u001b[43mfunc_graph_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction_type_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m transform.apply_func_graph_transforms(traced_func_graph)\n\u001b[32m    324\u001b[39m graph_capture_container = traced_func_graph.function_captures\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1060\u001b[39m, in \u001b[36mfunc_graph_from_py_func\u001b[39m\u001b[34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[39m\n\u001b[32m   1057\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m   1059\u001b[39m _, original_func = tf_decorator.unwrap(python_func)\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m func_outputs = \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[32m   1064\u001b[39m func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[39m, in \u001b[36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m default_graph._variable_creator_scope(scope, priority=\u001b[32m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    596\u001b[39m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[32m    597\u001b[39m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[32m    598\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     out = \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[39m, in \u001b[36mpy_func_from_autograph.<locals>.autograph_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m     51\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mag_error_metadata\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339\u001b[39m, in \u001b[36mconverted_call\u001b[39m\u001b[34m(f, args, kwargs, caller_fn_scope, options)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[32m    338\u001b[39m   logging.log(\u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: AutoGraph artifact\u001b[39m\u001b[33m'\u001b[39m, f)\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools.partial):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[39m, in \u001b[36m_call_unconverted\u001b[39m\u001b[34m(f, args, kwargs, options, update_cache)\u001b[39m\n\u001b[32m    456\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f.\u001b[34m__self__\u001b[39m.call(args, kwargs)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m f(*args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:114\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mone_step_on_data\u001b[39m(data):\n\u001b[32m    113\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     outputs = reduce_per_replica(\n\u001b[32m    116\u001b[39m         outputs,\n\u001b[32m    117\u001b[39m         \u001b[38;5;28mself\u001b[39m.distribute_strategy,\n\u001b[32m    118\u001b[39m         reduction=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    119\u001b[39m     )\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1673\u001b[39m, in \u001b[36mStrategyBase.run\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1668\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scope():\n\u001b[32m   1669\u001b[39m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[32m   1670\u001b[39m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[32m   1671\u001b[39m   fn = autograph.tf_convert(\n\u001b[32m   1672\u001b[39m       fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1673\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extended\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3263\u001b[39m, in \u001b[36mStrategyExtendedV1.call_for_each_replica\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   3261\u001b[39m   kwargs = {}\n\u001b[32m   3262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3263\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4061\u001b[39m, in \u001b[36m_DefaultDistributionExtended._call_for_each_replica\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   4059\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[32m   4060\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m._container_strategy(), replica_id_in_sync_group=\u001b[32m0\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4061\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:78\u001b[39m, in \u001b[36mTensorFlowTrainer.train_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainable_weights:\n\u001b[32m     77\u001b[39m     trainable_weights = \u001b[38;5;28mself\u001b[39m.trainable_weights\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     gradients = \u001b[43mtape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.apply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, trainable_weights))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1066\u001b[39m, in \u001b[36mGradientTape.gradient\u001b[39m\u001b[34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[39m\n\u001b[32m   1060\u001b[39m   output_gradients = (\n\u001b[32m   1061\u001b[39m       composite_tensor_gradient.get_flat_tensors_for_gradients(\n\u001b[32m   1062\u001b[39m           output_gradients))\n\u001b[32m   1063\u001b[39m   output_gradients = [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops.convert_to_tensor(x)\n\u001b[32m   1064\u001b[39m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m flat_grad = \u001b[43mimperative_grad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m=\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._persistent:\n\u001b[32m   1075\u001b[39m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[32m   1076\u001b[39m   \u001b[38;5;28mself\u001b[39m._watched_variables = \u001b[38;5;28mself\u001b[39m._tape.watched_variables()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[39m, in \u001b[36mimperative_grad\u001b[39m\u001b[34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m     64\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     65\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % unconnected_gradients)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:148\u001b[39m, in \u001b[36m_gradient_function\u001b[39m\u001b[34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[39m\n\u001b[32m    146\u001b[39m     gradient_name_scope += forward_pass_name_scope + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(gradient_name_scope):\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, *out_grads)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:866\u001b[39m, in \u001b[36m_TransposeGrad\u001b[39m\u001b[34m(op, grad)\u001b[39m\n\u001b[32m    864\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns unshuffle(grad).\"\"\"\u001b[39;00m\n\u001b[32m    865\u001b[39m p = op.inputs[\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [array_ops.transpose(grad, \u001b[43marray_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvert_permutation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:4696\u001b[39m, in \u001b[36minvert_permutation\u001b[39m\u001b[34m(x, name)\u001b[39m\n\u001b[32m   4694\u001b[39m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[32m   4695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4696\u001b[39m   _, _, _op, _outputs = \u001b[43m_op_def_library\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4697\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mInvertPermutation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4698\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   4699\u001b[39m   _result = _dispatch.dispatch(\n\u001b[32m   4700\u001b[39m         invert_permutation, (), \u001b[38;5;28mdict\u001b[39m(x=x, name=name)\n\u001b[32m   4701\u001b[39m       )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:796\u001b[39m, in \u001b[36m_apply_op_helper\u001b[39m\u001b[34m(op_type_name, name, **keywords)\u001b[39m\n\u001b[32m    791\u001b[39m must_colocate_inputs = [val \u001b[38;5;28;01mfor\u001b[39;00m arg, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(op_def.input_arg, inputs)\n\u001b[32m    792\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m arg.is_ref]\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[32m    794\u001b[39m   \u001b[38;5;66;03m# Add Op to graph\u001b[39;00m\n\u001b[32m    795\u001b[39m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m   op = \u001b[43mg\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattr_protos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m=\u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;66;03m# for more details.\u001b[39;00m\n\u001b[32m    804\u001b[39m outputs = op.outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:614\u001b[39m, in \u001b[36mFuncGraph._create_op_internal\u001b[39m\u001b[34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[39m\n\u001b[32m    612\u001b[39m   inp = \u001b[38;5;28mself\u001b[39m.capture(inp)\n\u001b[32m    613\u001b[39m   captured_inputs.append(inp)\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2726\u001b[39m, in \u001b[36mGraph._create_op_internal\u001b[39m\u001b[34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[39m\n\u001b[32m   2723\u001b[39m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[32m   2724\u001b[39m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[32m   2725\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mutation_lock():\n\u001b[32m-> \u001b[39m\u001b[32m2726\u001b[39m   ret = \u001b[43mOperation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_node_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2734\u001b[39m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m=\u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2735\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2736\u001b[39m   \u001b[38;5;28mself\u001b[39m._create_op_helper(ret, compute_device=compute_device)\n\u001b[32m   2737\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1221\u001b[39m, in \u001b[36mOperation.from_node_def\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1218\u001b[39m     control_input_ops.append(control_op)\n\u001b[32m   1220\u001b[39m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m c_op = \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m=\u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[38;5;28mself\u001b[39m = Operation(c_op, SymbolicTensor)\n\u001b[32m   1223\u001b[39m \u001b[38;5;28mself\u001b[39m._init(g)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adminuser\\Projects\\LSTM_Autoencoder_detection\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1078\u001b[39m, in \u001b[36m_create_c_op\u001b[39m\u001b[34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[39m\n\u001b[32m   1074\u001b[39m   pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),\n\u001b[32m   1075\u001b[39m                                          serialized)\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m   c_op = \u001b[43mpywrap_tf_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTF_FinishOperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_desc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m errors.InvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1080\u001b[39m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[32m   1081\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e.message)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, scaler, threshold, feature_columns, results = main()\n",
    "    \n",
    "    # Actually save the trained model for future use\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ’¾ SAVING MODEL FOR DEPLOYMENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    import joblib\n",
    "    \n",
    "    # Save Keras model\n",
    "    model.save('compressor_autoencoder2.h5')\n",
    "    print(\"âœ“ Model saved: compressor_autoencoder2.h5\")\n",
    "    \n",
    "    # Save scaler\n",
    "    joblib.dump(scaler, 'scaler2.pkl')\n",
    "    print(\"âœ“ Scaler saved: scaler2.pkl\")\n",
    "    \n",
    "    # Save threshold and feature names\n",
    "    joblib.dump({'threshold': threshold, 'features': feature_columns}, 'model_config2.pkl')\n",
    "    print(\"âœ“ Config saved: model_config2.pkl\")\n",
    "    \n",
    "    print(\"\\nğŸ“¦ Model package ready for deployment!\")\n",
    "    print(\"\\nTo load and use later:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from tensorflow.keras.models import load_model\")\n",
    "    print(\"import joblib\")\n",
    "    print(\"\")\n",
    "    print(\"model = load_model('compressor_autoencoder2.h5')\")\n",
    "    print(\"scaler = joblib.load('scaler2.pkl')\")\n",
    "    print(\"config = joblib.load('model_config2.pkl')\")\n",
    "    print(\"threshold = config['threshold']\")\n",
    "    print(\"feature_columns = config['features']\")\n",
    "    print(\"```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525df1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b9be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
